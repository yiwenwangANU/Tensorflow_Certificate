{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow Certificate Model 0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQytG0lykg9wFOJhT5r43z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiwenwangANU/Tensorflow_Certificate/blob/main/Tensorflow_Certificate_Model_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "o0r_HxCC4FcJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZRvDZNYxOJs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "UYgw4fcDx8sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_to_file, 'rb') as f:\n",
        "  raw_data = f.read().decode('utf-8')"
      ],
      "metadata": {
        "id": "RtDqTY7WyAY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(raw_data))\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5eUFNO7yMjx",
        "outputId": "fa5c0e16-218a-42b4-f7c2-4205ceeae877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_chars = tf.strings.unicode_split(raw_data, input_encoding='UTF-8')\n",
        "all_chars[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Y1nblmyOKG",
        "outputId": "810bebae-1f09-4103-8706-9da3fdeb4b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100,), dtype=string, numpy=\n",
              "array([b'F', b'i', b'r', b's', b't', b' ', b'C', b'i', b't', b'i', b'z',\n",
              "       b'e', b'n', b':', b'\\n', b'B', b'e', b'f', b'o', b'r', b'e', b' ',\n",
              "       b'w', b'e', b' ', b'p', b'r', b'o', b'c', b'e', b'e', b'd', b' ',\n",
              "       b'a', b'n', b'y', b' ', b'f', b'u', b'r', b't', b'h', b'e', b'r',\n",
              "       b',', b' ', b'h', b'e', b'a', b'r', b' ', b'm', b'e', b' ', b's',\n",
              "       b'p', b'e', b'a', b'k', b'.', b'\\n', b'\\n', b'A', b'l', b'l', b':',\n",
              "       b'\\n', b'S', b'p', b'e', b'a', b'k', b',', b' ', b's', b'p', b'e',\n",
              "       b'a', b'k', b'.', b'\\n', b'\\n', b'F', b'i', b'r', b's', b't', b' ',\n",
              "       b'C', b'i', b't', b'i', b'z', b'e', b'n', b':', b'\\n', b'Y', b'o',\n",
              "       b'u'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_to_ids = layers.StringLookup(vocabulary=vocab)\n",
        "ids_to_chars = layers.StringLookup(vocabulary=chars_to_ids.get_vocabulary(), invert=True)\n",
        "all_ids = chars_to_ids(all_chars)\n",
        "all_ids[:100],len(all_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp2s3ihnzBVF",
        "outputId": "b20c0cda-5029-4541-d288-7aef1221dd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
              " array([38, 10, 29, 46, 11,  1, 62, 10, 11, 10, 39, 56,  2, 42, 40, 48, 56,\n",
              "        49, 16, 29, 56,  1, 59, 56,  1, 23, 29, 16, 60, 56, 56, 34,  1,  6,\n",
              "         2, 21,  1, 49, 57, 29, 11,  9, 56, 29, 35,  1,  9, 56,  6, 29,  1,\n",
              "        17, 56,  1, 46, 23, 56,  6, 43, 26, 40, 40, 64, 51, 51, 42, 40,  7,\n",
              "        23, 56,  6, 43, 35,  1, 46, 23, 56,  6, 43, 26, 40, 40, 38, 10, 29,\n",
              "        46, 11,  1, 62, 10, 11, 10, 39, 56,  2, 42, 40, 30, 16, 57])>, 1115394)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "ids_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvlEiMAczgo6",
        "outputId": "16b8cef4-3a39-4ee5-abd0-caa7e66b38a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "batched_dataset = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "batched_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znR8fT611Wt7",
        "outputId": "886e32d5-0134-403f-8950-ffe8dd6e264e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=TensorSpec(shape=(101,), dtype=tf.int64, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def input_target_split(sequence):\n",
        "  return sequence[:-1], sequence[1:]"
      ],
      "metadata": {
        "id": "aEA3H3wa2Wqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = batched_dataset.map(input_target_split)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jKrDms72wKe",
        "outputId": "2bd4aa2d-4ba5-4569-c949-5661918316e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, target in dataset.take(1):\n",
        "  print(tf.strings.reduce_join(ids_to_chars(inputs)))\n",
        "  print(tf.strings.reduce_join(ids_to_chars(target)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxB4v2zC3IvQ",
        "outputId": "52e981d3-6ccc-4f3a-a063-9cf0a930f1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou', shape=(), dtype=string)\n",
            "tf.Tensor(b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################forget###############################\n",
        "prefetched_dataset = dataset.batch(64).prefetch(tf.data.AUTOTUNE) \n",
        "prefetched_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj_HcMpy8Swr",
        "outputId": "2ed0e977-cd8a-40fb-f73c-9328b84542ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 100), dtype=tf.int64, name=None), TensorSpec(shape=(None, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "3WY6Eu_f3hgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars_to_ids.get_vocabulary())\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "jtikMXFV5IFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model_0(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dims, rnn_units):\n",
        "        super(Model_0, self).__init__()\n",
        "        self.embed = layers.Embedding(input_dim=vocab_size,\n",
        "                                      output_dim=embedding_dims,\n",
        "                                      name='embed')\n",
        "        self.LSTM = layers.LSTM(units=rnn_units, ###############################?????###############################\n",
        "                                return_sequences=True,\n",
        "                                return_state=True,\n",
        "                                name='LSTM')\n",
        "        self.Dense = layers.Dense(units=vocab_size, name='Dense')\n",
        "\n",
        "    def call(self, inputs, return_state=False, fm_state=None, fc_state=None):\n",
        "        x = self.embed(inputs)  #(batch, 100, embed)\n",
        "\n",
        "        if(fm_state==None):\n",
        "          fm_state, fc_state = self.LSTM.get_initial_state(x)\n",
        "        x, fm_state, fc_state = self.LSTM(x, initial_state=[fm_state, fc_state])  #(batch, seq, vocab)\n",
        "        outputs = self.Dense(x)\n",
        "        if(return_state==True):\n",
        "          return outputs, fm_state, fc_state\n",
        "        else:\n",
        "          return outputs\n"
      ],
      "metadata": {
        "id": "41WlftYk4HwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model_0(vocab_size=vocab_size,\n",
        "                embedding_dims=embedding_dims,\n",
        "                rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "XAJ3yJei7xdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, _ in prefetched_dataset.take(1):\n",
        "  print(tf.strings.reduce_join(ids_to_chars(inputs[0])))\n",
        "  ###############################forget###############################\n",
        "  predicted_ids = tf.squeeze(tf.random.categorical(model(inputs)[0], 1))\n",
        "  print(tf.strings.reduce_join(ids_to_chars(predicted_ids)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byv8j-Kg7cJr",
        "outputId": "b2be7bda-e88c-43b2-868d-6b50857a9e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou', shape=(), dtype=string)\n",
            "tf.Tensor(b'VF$armHDkPM&h:,L!Rlrl::N&,rC$l;m\\nn:pt&cRFH,WnTxdsnpb!yMiirpZ-V3eEb.hw-WEEpJuq?bMeFfAbkJc:QqYGADDv\\nUn', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam())"
      ],
      "metadata": {
        "id": "gIyGov807__r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(prefetched_dataset,\n",
        "                    epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foUInOcoGQOr",
        "outputId": "0a061305-eab0-4ae5-a687-ede0d967567a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "173/173 [==============================] - 14s 69ms/step - loss: 2.8480\n",
            "Epoch 2/20\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 2.1567\n",
            "Epoch 3/20\n",
            "173/173 [==============================] - 13s 77ms/step - loss: 1.9179\n",
            "Epoch 4/20\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 1.7526\n",
            "Epoch 5/20\n",
            "173/173 [==============================] - 13s 72ms/step - loss: 1.6269\n",
            "Epoch 6/20\n",
            "173/173 [==============================] - 13s 72ms/step - loss: 1.5330\n",
            "Epoch 7/20\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 1.4634\n",
            "Epoch 8/20\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 1.4092\n",
            "Epoch 9/20\n",
            "173/173 [==============================] - 13s 74ms/step - loss: 1.3647\n",
            "Epoch 10/20\n",
            "173/173 [==============================] - 13s 75ms/step - loss: 1.3262\n",
            "Epoch 11/20\n",
            "173/173 [==============================] - 13s 75ms/step - loss: 1.2916\n",
            "Epoch 12/20\n",
            "173/173 [==============================] - 13s 73ms/step - loss: 1.2594\n",
            "Epoch 13/20\n",
            "173/173 [==============================] - 13s 74ms/step - loss: 1.2280\n",
            "Epoch 14/20\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 1.1971\n",
            "Epoch 15/20\n",
            "173/173 [==============================] - 13s 74ms/step - loss: 1.1667\n",
            "Epoch 16/20\n",
            "173/173 [==============================] - 13s 74ms/step - loss: 1.1376\n",
            "Epoch 17/20\n",
            "173/173 [==============================] - 13s 73ms/step - loss: 1.1121\n",
            "Epoch 18/20\n",
            "173/173 [==============================] - 13s 73ms/step - loss: 1.0831\n",
            "Epoch 19/20\n",
            "173/173 [==============================] - 13s 76ms/step - loss: 1.0505\n",
            "Epoch 20/20\n",
            "173/173 [==============================] - 13s 73ms/step - loss: 1.0191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, _ in prefetched_dataset.take(1):\n",
        "  print(tf.strings.reduce_join(ids_to_chars(inputs[0])))\n",
        "  predicted_ids = tf.squeeze(tf.random.categorical(model(inputs)[0], 1))\n",
        "  print(tf.strings.reduce_join(ids_to_chars(predicted_ids)))"
      ],
      "metadata": {
        "id": "QfH7VlpAGa_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e28f13c-70cc-46e5-842b-f5d867a9345f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou', shape=(), dtype=string)\n",
            "tf.Tensor(b'orst Hitizen:\\nFugore terpeoueed snd trrther  goar me.spaak,\\n\\nENl \\nHeeak  sheak \\n\\nMirst Sitizen:\\nPou ', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs='ROMEO:'\n",
        "#()\n",
        "input_ids = chars_to_ids(tf.strings.unicode_split(inputs, input_encoding='UTF-8')) #(seq)\n",
        "output, state = model(tf.expand_dims(input_ids,axis=0), return_state=True) #(batch, seq, vocab)\n",
        "output = tf.squeeze(output, axis=0) #(seq, vocab)\n",
        "pred_ids = tf.squeeze(tf.random.categorical(output, 1), axis=-1) #(seq)\n",
        "pred_chars = ids_to_chars(pred_ids)  #(seq)\n",
        "last_char = pred_chars[-1]"
      ],
      "metadata": {
        "id": "GB0LB6TeKZaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model=model, inputs='ROMEO:', fm_state=None, fc_state=None):\n",
        "  input_ids = chars_to_ids(tf.strings.unicode_split(inputs, input_encoding='UTF-8')) #(seq)\n",
        "  output, fm_state, fc_state = model(tf.expand_dims(input_ids,axis=0),\n",
        "                                return_state=True,\n",
        "                                fm_state=fm_state,\n",
        "                                fc_state=fc_state) #(batch, seq, vocab)  ###############################forget###############################\n",
        "  output = tf.squeeze(output, axis=0) #(seq, vocab)\n",
        "  pred_ids = tf.squeeze(tf.random.categorical(output, 1), axis=-1) #(seq)\n",
        "  pred_chars = ids_to_chars(pred_ids)  #(seq)\n",
        "  next_word = pred_chars[-1]\n",
        "  return next_word, fm_state, fc_state"
      ],
      "metadata": {
        "id": "847b1M8pG7EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_word(inputs='R')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na4kKiX1MsUb",
        "outputId": "e495046c-d5ae-45fd-aa2d-1f790cef90d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=string, numpy=b'b'>,\n",
              " <tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
              " array([[ 0.06281871, -0.05333113, -0.03771185, ..., -0.01375718,\n",
              "          0.03989689, -0.1401    ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
              " array([[ 0.40222946, -0.15128659, -0.06906644, ..., -0.0177927 ,\n",
              "          0.04723381, -0.5599148 ]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(model=model, initial_inputs='ROMEO:', fm_state=None, fc_state=None, steps=1000):\n",
        "  output = [initial_inputs]\n",
        "  next_word=initial_inputs\n",
        "  for i in range(steps):\n",
        "    next_word, fm_state, fc_state = predict_next_word(inputs=next_word,\n",
        "                                                      fm_state=fm_state,\n",
        "                                                      fc_state=fc_state)\n",
        "    output.append(next_word)\n",
        "  return tf.strings.reduce_join(output)"
      ],
      "metadata": {
        "id": "971la0A3M2J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = make_prediction()\n",
        "print(predictions.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOb-kziiOXGB",
        "outputId": "3fec9c6c-6800-4829-8291-ecb12cb46c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Poor some better, I say, I hardly\n",
            "the mounted with the sea to the first\n",
            "Who deserves intit the womb at his deposing?\n",
            "\n",
            "Will I:\n",
            "What, how now! who's as him toncume?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "What, how she shall be wetchmen innocent!\n",
            "\n",
            "USTiNTIUS:\n",
            "Cannot but sweet and rose:\n",
            "Whilst I't! aw! then for me send thy word.\n",
            "\n",
            "CAMFID:\n",
            "Nevel, thusses hold; my son Petruchio\n",
            "Petrour or extermil.\n",
            "\n",
            "HORTENSIO:\n",
            "Farewell; and see at thy sword and learn!\n",
            "O, peace! I have pissed towards:\n",
            "saw now, sir; well, sirPall and now go melt to have\n",
            "A little gnazed up mes-ecced.\n",
            "\n",
            "BRUTUS:\n",
            "O, sir, and bring to bear which he slew\n",
            "From him that heavy with Kemporr'd,\n",
            "When he shall have me in myself.\n",
            "\n",
            "ANTONIO:\n",
            "Hath eit regain'd how I'll cun, or told you out:\n",
            "What, prove is on enter thing we would say they\n",
            "Were chairls they love to love; he shall decive\n",
            "As bying a chambel and some seven pent\n",
            "To take her strive Been Cray. But, sirr, sir!\n",
            "\n",
            "SEBASTIAN:\n",
            "How now, sir! have not andiving began?\n",
            "\n",
            "BIRTAPO:\n",
            "Ripinion, not a carm, with themselvs.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}